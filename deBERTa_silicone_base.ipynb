{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1391de7-402c-473d-a865-e9f6af57c32c",
   "metadata": {},
   "source": [
    "# Etude du Dataset SILICONE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9282b8a3-4945-49b5-98ec-04cc7bb4ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline, Trainer, TrainingArguments\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tasknet import Adapter\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from CustomTrainer import CustomTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c7ac0-6c8f-4efc-a0e3-c080b6b5457f",
   "metadata": {},
   "source": [
    "# Chargement du Dataset SILICONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0b8e6e7-bc12-474d-81fd-bed6cfba08bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset silicone (C:/Users/robin/.cache/huggingface/datasets/silicone/dyda_da/1.0.0/af617406c94e3f78da85f7ea74ebfbd3f297a9665cb54adbae305b03bc4442a5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0434aedaa9a1412ca70d3247590ea23c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load SILICONE dataset in its Dyda_da config (only 4 possible dialog acts)\n",
    "\n",
    "silicone_dyda = load_dataset('silicone', 'dyda_da')\n",
    "dyda_train = silicone_dyda['train']\n",
    "dyda_valid = silicone_dyda['validation']\n",
    "dyda_test = silicone_dyda['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4da9b35-8764-4ea9-8309-3d9e19444840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Utterance', 'Dialogue_Act', 'Dialogue_ID', 'Label', 'Idx'],\n",
       "    num_rows: 87170\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dyda_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5025f95a-3610-4c42-bc86-9bcdbd503f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 4\n",
    "labels = ['commissive', 'directive', 'inform', 'question']\n",
    "id2label = {0: 'commissive',\n",
    "            1: 'directive', \n",
    "            2: 'inform', \n",
    "            3: 'question'\n",
    "}\n",
    "label2id = {value: key for key, value in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "825ff4c3-baef-44ec-85a7-b76a383dfffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Dialogue_Act</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hey man , you wanna buy some weed ?</td>\n",
       "      <td>directive</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>some what ?</td>\n",
       "      <td>question</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weed ! you know ? pot , ganja , mary jane some...</td>\n",
       "      <td>directive</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oh , umm , no thanks .</td>\n",
       "      <td>commissive</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i also have blow if you prefer to do a few lin...</td>\n",
       "      <td>directive</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Utterance Dialogue_Act Dialogue_ID  \\\n",
       "0                hey man , you wanna buy some weed ?    directive           1   \n",
       "1                                        some what ?     question           1   \n",
       "2  weed ! you know ? pot , ganja , mary jane some...    directive           1   \n",
       "3                             oh , umm , no thanks .   commissive           1   \n",
       "4  i also have blow if you prefer to do a few lin...    directive           1   \n",
       "\n",
       "   Label  Idx  \n",
       "0      1    0  \n",
       "1      3    1  \n",
       "2      1    2  \n",
       "3      0    3  \n",
       "4      1    4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dyda_train_df = pd.DataFrame.from_dict(dyda_train[:])\n",
    "dyda_test_df = pd.DataFrame.from_dict(dyda_test[:])\n",
    "dyda_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5de462-1f98-46f2-be0b-76b5a574f5eb",
   "metadata": {},
   "source": [
    "# Chargement du modèle préentraîné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb09fbb-1885-41ac-94f5-da545758457a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "C:\\Users\\robin\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_name = 'microsoft/deberta-v3-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, \n",
    "    ignore_mismatched_sizes=True,\n",
    "    num_labels=num_labels, \n",
    "    #id2label=id2label, label2id=label2id\n",
    ").to(device)\n",
    "\n",
    "classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae9096a-a384-4e30-9797-8101c1eac05f",
   "metadata": {},
   "source": [
    "# First Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbd0ce20-bb08-4ff1-b0e8-ea658728d234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81bda727a49040fb8797eb5b44e60ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057cc554ef4a412facb91d23649db701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"Utterance\"], truncation=True, max_length=128)\n",
    "\n",
    "valid_tkz = dyda_valid.map(tokenize_function, batched=True)\n",
    "train_tkz = dyda_train.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ef2ee1-81c9-43ae-b4a0-e1c2c0ac39b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Utterance', 'Dialogue_Act', 'Dialogue_ID', 'Label', 'Idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 87170\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tkz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a03f4517-1343-4b05-bc24-4531e87e947d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 16, 14, 27, 24, 23, 32, 31]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "samples = train_tkz[:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"Utterance\", \"Dialogue_Act\", \"Idx\", \"Dialogue_ID\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dc6d730-d58c-41bc-8df5-4491240a14dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Label': torch.Size([8]),\n",
       " 'input_ids': torch.Size([8, 32]),\n",
       " 'token_type_ids': torch.Size([8, 32]),\n",
       " 'attention_mask': torch.Size([8, 32])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc97457e-f460-4806-9805-9eb19a9e84fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = train_tkz[:200]\n",
    "train_sample = {k: v for k, v in train_sample.items() if k not in [\"Utterance\", \"Dialogue_Act\", \"Idx\", \"Dialogue_ID\"]}\n",
    "\n",
    "valid_sample = valid_tkz[:100]\n",
    "valid_sample = {k: v for k, v in valid_sample.items() if k not in [\"Utterance\", \"Dialogue_Act\", \"Idx\", \"Dialogue_ID\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a213b53-87d7-4641-a8a5-89f99cd104f9",
   "metadata": {},
   "source": [
    "# Preprocessing V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7dbf7ad-fef6-4956-82d5-4e7360fbb968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build smaller dataset\n",
    "\n",
    "dyda_train_sample = dyda_train[100:300]\n",
    "dyda_valid_sample = dyda_valid[:50]\n",
    "dyda_test_sample = dyda_test[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fde492a-74d8-49bb-913a-6b82ce7ec436",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = tokenizer(dyda_train_sample['Utterance'], padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "valid_inputs = tokenizer(dyda_valid_sample['Utterance'], padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "test_inputs = tokenizer(dyda_test_sample['Utterance'], padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "#train_labels = tokenizer(dyda_train['Dialogue_Act'], padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "#valid_labels = tokenizer(dyda_valid['Dialogue_Act'], padding='max_length', truncation=True, max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9c77c3e-bfd4-4d1c-ad06-4cd878cdf248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   584,   700,  ...,     0,     0,     0],\n",
       "        [    1,   584,   507,  ...,     0,     0,     0],\n",
       "        [    1,   262, 20497,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    1,   262,  1493,  ...,     0,     0,     0],\n",
       "        [    1,   584,   398,  ...,     0,     0,     0],\n",
       "        [    1,   278,   507,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1ee2c8b-4cf8-4395-a4d3-c7c20f7c70f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorDataset from the input data and labels\n",
    "train_dataset = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_inputs['token_type_ids'], torch.tensor(dyda_train_sample['Label']))\n",
    "valid_dataset = TensorDataset(valid_inputs['input_ids'], valid_inputs['attention_mask'], valid_inputs['token_type_ids'], torch.tensor(dyda_valid_sample['Label']))\n",
    "test_dataset = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_inputs['token_type_ids'], torch.tensor(dyda_test_sample['Label']))\n",
    "\n",
    "# Create DataLoader objects for the training and validation sets\n",
    "#train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "#valid_loader = DataLoader(valid_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ab0263-93fc-4b7e-b088-f5d1789c8182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe42f221-2d09-4a8f-92ac-1dc607841809",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d476a887-0582-45b1-8067-9734f89965ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robin\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 200\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 39\n",
      "  Number of trainable parameters = 184425220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels found\n",
      "tensor(1.3349, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 07:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels found\n",
      "tensor(1.3106, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.3428, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.3532, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.3179, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.3163, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.3189, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.3302, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.3147, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.3202, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.3090, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.3169, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.3316, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.3165, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.3175, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.3173, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.2985, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.2816, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.3164, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.2907, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.3107, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.3040, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.2855, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.2924, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.2891, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.3037, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.2959, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.2797, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.3017, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.2808, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.2600, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.2515, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.2616, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.2877, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.2382, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.2586, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.3018, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.3051, grad_fn=<NllLossBackward0>)\n",
      "Labels found\n",
      "tensor(1.2599, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39, training_loss=1.3006011278201373, metrics={'train_runtime': 458.7101, 'train_samples_per_second': 1.308, 'train_steps_per_second': 0.085, 'total_flos': 39468074803200.0, 'train_loss': 1.3006011278201373, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=50,\n",
    "    save_total_limit=2,\n",
    "    save_steps=50\n",
    ")\n",
    "\n",
    "# define the trainer object\n",
    "trainer = CustomTrainer(\n",
    "    model=model,                     # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                     # training arguments, defined above\n",
    "    train_dataset=train_dataset,            # training dataset\n",
    "    eval_dataset=valid_dataset,               # evaluation dataset\n",
    "    #compute_metrics=lambda pred, labels: {\"accuracy\": accuracy_score(labels, pred.argmax(axis=1))},\n",
    "    #data_collator=data_collator,\n",
    "    #tokenizer=tokenizer,\n",
    "    data_collator=lambda data: {'input_ids': torch.stack([item[0] for item in data]),\n",
    "                                'attention_mask': torch.stack([item[1] for item in data]),\n",
    "                                'token_type_ids': torch.stack([item[2] for item in data]),\n",
    "                                'labels': torch.tensor([item[3] for item in data])},\n",
    ")\n",
    "\n",
    "# start the training process\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94279740-94e3-47d9-b961-042b9e80f068",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9117fabc-07db-4393-94b9-9b158430736e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels found\n",
      "tensor(1.2455)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels found\n",
      "tensor(1.2763)\n",
      "(100, 4) (100,)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset)\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07cd2f6c-900e-4e37-a371-7f678cbb659f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.38}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "accuracy.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5f62de-1580-4689-8cf9-8abbe226eefb",
   "metadata": {},
   "source": [
    "On passe de 0.31 à 0.38 d'accuracy en finetunant sur 200 utterances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57c0486-fc1f-4cfc-b295-43a6895ed26a",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9182780d-c4ef-4ff4-8cdc-7b4334bf9819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels found\n",
      "tensor(1.2366)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.2365946769714355,\n",
       " 'eval_runtime': 10.3575,\n",
       " 'eval_samples_per_second': 4.827,\n",
       " 'eval_steps_per_second': 0.097,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e78ab1f-ccaa-4082-bd72-92e2308c5cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "\n",
    "#model_inputs_test = tokenizer(dyda_test_sample['Utterance'], padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "model_inputs_test = test_inputs['input_ids'].tolist()\n",
    "preds = []\n",
    "for input in model_inputs_test:\n",
    "    pred = model(torch.tensor([input]))\n",
    "    preds.append(pred)\n",
    "#preds = model(model_inputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec2c864c-0e06-4ff7-aa31-933d8ad11f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SequenceClassifierOutput(loss=None, logits=tensor([[-0.1522, -0.1729,  0.0872,  0.0323]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=None, logits=tensor([[-0.2559, -0.1825, -0.1750,  0.0066]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=None, logits=tensor([[-0.3809, -0.1858, -0.0521,  0.0344]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=None, logits=tensor([[-0.3262, -0.0846, -0.0612, -0.0484]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=None, logits=tensor([[-0.2262, -0.1908,  0.0267, -0.0462]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)]\n"
     ]
    }
   ],
   "source": [
    "print(preds[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c3d43fc-e8c4-4c52-bd2f-5d22841bf854",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CustomTrainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mCustomTrainer\u001b[49m(\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                     \u001b[38;5;66;03m# the instantiated 🤗 Transformers model to be trained\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                     \u001b[38;5;66;03m# training arguments, defined above\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,            \u001b[38;5;66;03m# training dataset\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mvalid_dataset,               \u001b[38;5;66;03m# evaluation dataset\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#compute_metrics=lambda pred, labels: {\"accuracy\": accuracy_score(labels, pred.argmax(axis=1))},\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#data_collator=data_collator,\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m#tokenizer=tokenizer,\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m data: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mstack([item[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data]),\n\u001b[0;32m     12\u001b[0m                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mstack([item[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data]),\n\u001b[0;32m     13\u001b[0m                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mstack([item[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data]),\n\u001b[0;32m     14\u001b[0m                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([item[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data])},\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m trainer\u001b[38;5;241m.\u001b[39mpredict(test_dataset)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CustomTrainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,                     # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                     # training arguments, defined above\n",
    "    train_dataset=train_dataset,            # training dataset\n",
    "    eval_dataset=valid_dataset,               # evaluation dataset\n",
    "    #compute_metrics=lambda pred, labels: {\"accuracy\": accuracy_score(labels, pred.argmax(axis=1))},\n",
    "    #data_collator=data_collator,\n",
    "    #tokenizer=tokenizer,\n",
    "    data_collator=lambda data: {'input_ids': torch.stack([item[0] for item in data]),\n",
    "                                'attention_mask': torch.stack([item[1] for item in data]),\n",
    "                                'token_type_ids': torch.stack([item[2] for item in data]),\n",
    "                                'labels': torch.tensor([item[3] for item in data])},\n",
    ")\n",
    "\n",
    "trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646d8aba-df54-491c-939a-c22e8e055bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model explainability (confusion matrix) ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
